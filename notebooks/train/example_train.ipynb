{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659a02b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thang/MasterDegree/Aalto/Study/NaturalLanguageProcessing/Project/ToxicityDetection/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 236/236 [00:00<00:00, 507.20it/s, Materializing param=norm.weight]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 1/1\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00, 12.04it/s] 1.86it/s, loss=3.02, tox_loss=0.612, lang_loss=2.41, labeled=1, step=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Macro F1: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.93it/s] 3.11s/it, loss=2, tox_loss=1.36, lang_loss=0.644, labeled=1, step=1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Macro F1: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00,  9.92it/s] 3.38s/it, loss=6.29, tox_loss=0.531, lang_loss=5.76, labeled=1, step=2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Macro F1: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.00it/s] 2.78s/it, loss=13.6, tox_loss=0, lang_loss=13.6, labeled=0, step=3]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Macro F1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00,  8.33it/s] 2.44s/it, loss=1.83, tox_loss=0, lang_loss=1.83, labeled=0, step=4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation - Macro F1: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 (λ=1.000): 100%|██████████| 10/10 [00:33<00:00,  3.39s/it, loss=15, tox_loss=0.0737, lang_loss=15, labeled=1, step=5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training - Loss: 4.8503, Toxicity Loss: 0.2576, Language Loss: 4.5927, Labeled Samples: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:00<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "  Loss: 22.3908\n",
      "  Toxicity Loss: 3.7195\n",
      "  Language Loss: 18.6714\n",
      "  Toxicity Macro F1: 0.0000\n",
      "  Labeled Samples: 4\n",
      "\n",
      "==================================================\n",
      "Training completed!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "from toxicity_detection.trainer import Trainer\n",
    "\n",
    "train_dataset = load_dataset(\"parquet\", data_files=\"../../data/train_combined.parquet\", split=\"train\")\n",
    "dev_dataset = load_dataset(\"parquet\", data_files=\"../../data/dev.parquet\", split=\"train\")\n",
    "languages = [\"en\", \"fi\", \"de\"]\n",
    "n_train_samples = 10\n",
    "n_dev_samples = 4\n",
    "\n",
    "def stratified_sample_equal_per_lang(dataset, languages, total_samples, seed=0):\n",
    "    \"\"\"Sample equally from each language in `languages`.\"\"\"\n",
    "    num_langs = len(languages)\n",
    "    base_n = total_samples // num_langs\n",
    "    remainder = total_samples % num_langs\n",
    "    per_lang_counts = {lang: base_n for lang in languages}\n",
    "    for lang in languages[:remainder]:\n",
    "        per_lang_counts[lang] += 1\n",
    "\n",
    "    per_lang_datasets = []\n",
    "    for lang in languages:\n",
    "        lang_subset = dataset.filter(lambda ex, l=lang: ex[\"lang\"] == l)\n",
    "        lang_subset = lang_subset.shuffle(seed=seed)\n",
    "        n = per_lang_counts[lang]\n",
    "        if n > len(lang_subset):\n",
    "            raise ValueError(f\"Requested {n} samples for language '{lang}', \"\n",
    "                            f\"but only {len(lang_subset)} available.\")\n",
    "        per_lang_datasets.append(lang_subset.select(range(n)))\n",
    "\n",
    "    combined = concatenate_datasets(per_lang_datasets).shuffle(seed=seed)\n",
    "    return combined\n",
    "\n",
    "train_samples = stratified_sample_equal_per_lang(\n",
    "    train_dataset, languages, total_samples=n_train_samples, seed=0\n",
    ")\n",
    "dev_samples = stratified_sample_equal_per_lang(\n",
    "    dev_dataset, languages, total_samples=n_dev_samples, seed=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_name=\"google/gemma-3-270m\",\n",
    "    train_dataset=train_samples,\n",
    "    val_dataset=dev_samples,\n",
    "    languages=[\"en\", \"fi\", \"de\"],\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=1,\n",
    "    accumulation_steps=2,\n",
    "    eval_steps=1,\n",
    "    num_epochs=1,\n",
    "    max_num_checkpoints=1\n",
    ")\n",
    "model, tokenizer = trainer.train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxicitydetection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
